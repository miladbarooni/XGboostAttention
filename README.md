# XGboostAttention
The paper, "Exploring the Integration of Self-Attention Mechanism and Gradient Boosted Trees Inspired by Vision Transformers," has been implemented.

In this paper, we tried to implement the self-attention mechanism with gradient-boosted trees (GBTs). We use the XGBoost library as an efficient implementation for GBTs. 

The following photo illustrates our model's overall architecture: 

![overall architecture](/home/milad/Downloads/XGboostAttention/images/OverallArch.png)

We've tested our model on MNIST and FashionMNIST, and the following image shows the attention learned by this architecture. 

![](/home/milad/Downloads/XGboostAttention/images/MNISTFashionMasks.png)
